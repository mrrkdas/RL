{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11208eff-9062-42af-a868-fde95ce3c9e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e59db2-9ab4-44bb-b7a0-8108fd461506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, truncated, terminated, info = env.step(action)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbb0742a-dbd6-4d97-9894-bee84fcd9a13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/Users/helloworld/anaconda3/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m      5\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m----> 6\u001b[0m     observation, reward, truncated, terminated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n\u001b[1;32m      9\u001b[0m         observation, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gym/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ale_py/env/gym.py:256\u001b[0m, in \u001b[0;36mAtariEnv.step\u001b[0;34m(self, action_ind)\u001b[0m\n\u001b[1;32m    254\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frameskip):\n\u001b[0;32m--> 256\u001b[0m     reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[1;32m    257\u001b[0m is_terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_over(with_truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    258\u001b[0m is_truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_truncated()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"ALE/Bowling-v5\", render_mode = \"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, truncated, terminated, info = env.step(action)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6130490-4035-471c-b6c5-12208be996fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[180, 122,  48],\n",
       "        [180, 122,  48],\n",
       "        [180, 122,  48],\n",
       "        ...,\n",
       "        [180, 122,  48],\n",
       "        [180, 122,  48],\n",
       "        [180, 122,  48]],\n",
       "\n",
       "       [[180, 122,  48],\n",
       "        [180, 122,  48],\n",
       "        [180, 122,  48],\n",
       "        ...,\n",
       "        [180, 122,  48],\n",
       "        [180, 122,  48],\n",
       "        [180, 122,  48]],\n",
       "\n",
       "       [[180, 122,  48],\n",
       "        [180, 122,  48],\n",
       "        [180, 122,  48],\n",
       "        ...,\n",
       "        [180, 122,  48],\n",
       "        [180, 122,  48],\n",
       "        [180, 122,  48]]], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "101f6ecd-ce02-4273-9e0a-c2f7338bf835",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lives': 0, 'episode_frame_number': 1024, 'frame_number': 1024}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562c063c-61f8-4eed-bbec-455ea962c9fd",
   "metadata": {},
   "source": [
    "# Observing Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8eb8f267-05b3-499b-a4f5-37ba5e212f1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Bowling-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b49a1be-6037-4221-b109-3d35c073cfc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "done = False\n",
    "observation, info = env.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "929c04e5-24bd-4d65-a4c6-d0495c870ee6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/helloworld/anaconda3/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:249: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "\n",
    "observation, reward, terminated,truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35c1b5c-d068-4fda-a86f-76034366719f",
   "metadata": {},
   "source": [
    "# Epsilon-Greedy Strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb6dd818-81b7-470a-8824-7a66cc9d9f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BowlingAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.95,\n",
    "    ):\n",
    "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
    "        of state-action values (q_values), a learning rate and an epsilon.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: The learning rate\n",
    "            initial_epsilon: The initial epsilon value\n",
    "            epsilon_decay: The decay for epsilon\n",
    "            final_epsilon: The final epsilon value\n",
    "            discount_factor: The discount factor for computing the Q-value\n",
    "        \"\"\"\n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.training_error = []\n",
    "\n",
    "    def get_action(self, obs: tuple[int, int, bool]) -> int:\n",
    "        \"\"\"\n",
    "        Returns the best action with probability (1 - epsilon)\n",
    "        otherwise a random action with probability epsilon to ensure exploration.\n",
    "        \"\"\"\n",
    "        # with probability epsilon return a random action to explore the environment\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "\n",
    "        # with probability (1 - epsilon) act greedily (exploit)\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        obs: tuple[int, int, bool],\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        terminated: bool,\n",
    "        next_obs: tuple[int, int, bool],\n",
    "    ):\n",
    "        \"\"\"Updates the Q-value of an action.\"\"\"\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
    "        temporal_difference = (\n",
    "            reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n",
    "        )\n",
    "\n",
    "        self.q_values[obs][action] = (\n",
    "            self.q_values[obs][action] + self.lr * temporal_difference\n",
    "        )\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbc1427b-0062-469e-a523-e6c378ba5fcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "learning_rate = 0.01\n",
    "n_episodes = 100_000\n",
    "start_epsilon = 1.0\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2)  # reduce the exploration over time\n",
    "final_epsilon = 0.1\n",
    "\n",
    "agent = BlackjackAgent(\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a20284a-46c9-4e64-8355-291cfbb64288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from gym.wrappers import RecordEpisodeStatistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3edbb850-0fca-483d-929e-4d357d590976",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/helloworld/Desktop/RL/Own.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m next_obs, reward, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# update the agent\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m agent\u001b[39m.\u001b[39mupdate(obs, action, reward, terminated, next_obs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# update if the environment is done and the current obs\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m done \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "\u001b[1;32m/Users/helloworld/Desktop/RL/Own.ipynb Cell 14\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     obs: \u001b[39mtuple\u001b[39m[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m, \u001b[39mbool\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     next_obs: \u001b[39mtuple\u001b[39m[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m, \u001b[39mbool\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m ):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Updates the Q-value of an action.\"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     future_q_value \u001b[39m=\u001b[39m (\u001b[39mnot\u001b[39;00m terminated) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_values[next_obs])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     temporal_difference \u001b[39m=\u001b[39m (\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m         reward \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscount_factor \u001b[39m*\u001b[39m future_q_value \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_values[obs][action]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_values[obs][action] \u001b[39m=\u001b[39m (\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_values[obs][action] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr \u001b[39m*\u001b[39m temporal_difference\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # play one episode\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # update the agent\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "        # update if the environment is done and the current obs\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "    agent.decay_epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb183b7-2cc6-46c4-bc85-3900cb97d117",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>, {})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defaultdict(lambda: np.zeros(env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f0560bd-1988-4bc0-8a16-f3b924df6802",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_blackjack = gym.make(\"Blackjack-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d904d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_bowling = gym.make(\"ALE/Bowling-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17b36b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(6)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_bowling.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0870f329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_blackjack.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28fed757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tuple(Discrete(32), Discrete(11), Discrete(2))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_blackjack.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cbc0ac40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (210, 160, 3), uint8)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_bowling.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e759d",
   "metadata": {},
   "source": [
    "# Deep Q-Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99c091b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3ac8a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.n_actions = action_size\n",
    "        self.lr = 0.001\n",
    "        self.gamma = 0.99\n",
    "        self.exploration_proba = 1.0\n",
    "        self.exploration_proba_decay = 0.005\n",
    "        self.batch_size = 32\n",
    "\n",
    "        self.memory_buffer = list()\n",
    "        self.max_memory_buffer = 2000\n",
    "\n",
    "        self.model =  Sequential([\n",
    "            Dense(24, input_shape = state_size, activation = 'relu'),\n",
    "            Dense(24, activation = 'relu'),\n",
    "            Dense(action_size, activation = 'linear')\n",
    "        ])\n",
    "\n",
    "        self.model.compile(loss=\"mse\", optimizer = Adam(lr=self.lr))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        def compute_action(self, current_state):\n",
    "            if np.random.uniform(0,1) < self.exploration_proba:\n",
    "                return np.random.choice(range(self.n_actions))\n",
    "            q_values = self.model.predict(current_state)\n",
    "            return np.argmax(q_values)\n",
    "        \n",
    "        def update_exploration_probability(self):\n",
    "            self.exploration_proba = self.exploration_proba * np.exp(-self.exploration_proba_decay) # epsilon-greedy formula\n",
    "\n",
    "        def store_episode(self, current_state, action, reward, next_state, done):\n",
    "            self.memory_buffer.append({\n",
    "                 \"current_state\": current_state,\n",
    "                \"action\":action,\n",
    "                \"reward\":reward,\n",
    "                \"next_state\":next_state,\n",
    "                \"done\" :done\n",
    "            })\n",
    "\n",
    "            if len(self.memory_buffer) > self.max_memory_buffer:\n",
    "                self.memory_buffer.pop()\n",
    "\n",
    "        def train(self):\n",
    "            np.random.shuffle(self.memory_buffer)\n",
    "            batch_sample = self.memory_buffer[0:self.batch_size]\n",
    "\n",
    "            for experience in batch_sample:\n",
    "                q_current_state = self.model.predict(experience[\"current_state\"])\n",
    "                q_target = experience[\"reward\"]\n",
    "\n",
    "                if not experience[\"done\"]:\n",
    "                    q_target = self.q_target + self.gamma*np.max(self.model.predict(experience[\"next_state\"])[0])\n",
    "                \n",
    "                q_current_state[0][experience[\"action\"]] = q_target\n",
    "\n",
    "                self.model.fit(experience[\"current_state\"], q_current_state, verbose = 0)\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96fc191c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/helloworld/Desktop/RL/Own.ipynb Cell 25\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m n_episodes \u001b[39m=\u001b[39m \u001b[39m400\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m max_iteration_ep \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m agent \u001b[39m=\u001b[39m DQNAgent(state_size, action_size)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m total_steps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_episodes):\n",
      "\u001b[1;32m/Users/helloworld/Desktop/RL/Own.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory_buffer \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_memory_buffer \u001b[39m=\u001b[39m \u001b[39m2000\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m  Sequential([\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     Dense(\u001b[39m24\u001b[39m, input_shape \u001b[39m=\u001b[39m state_size, activation \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     Dense(\u001b[39m24\u001b[39m, activation \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     Dense(action_size, activation \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m ])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m\"\u001b[39m, optimizer \u001b[39m=\u001b[39m Adam(lr\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_action\u001b[39m(\u001b[39mself\u001b[39m, current_state):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/dtensor/utils.py:96\u001b[0m, in \u001b[0;36mallow_initializer_layout.<locals>._wrap_function\u001b[0;34m(layer_instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[39mif\u001b[39;00m layout:\n\u001b[1;32m     94\u001b[0m             layout_args[variable_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_layout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m layout\n\u001b[0;32m---> 96\u001b[0m init_method(layer_instance, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     98\u001b[0m \u001b[39m# Inject the layout parameter after the invocation of __init__()\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39mfor\u001b[39;00m layout_param_name, layout \u001b[39min\u001b[39;00m layout_args\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:117\u001b[0m, in \u001b[0;36mDense.__init__\u001b[0;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39m@utils\u001b[39m\u001b[39m.\u001b[39mallow_initializer_layout\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    104\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    116\u001b[0m ):\n\u001b[0;32m--> 117\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(activity_regularizer\u001b[39m=\u001b[39mactivity_regularizer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    119\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munits \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(units) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(units, \u001b[39mint\u001b[39m) \u001b[39melse\u001b[39;00m units\n\u001b[1;32m    120\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munits \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/trackable/base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    205\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/engine/base_layer.py:452\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[0;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    451\u001b[0m             batch_size \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 452\u001b[0m         batch_input_shape \u001b[39m=\u001b[39m (batch_size,) \u001b[39m+\u001b[39m \u001b[39mtuple\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    453\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_input_shape \u001b[39m=\u001b[39m batch_input_shape\n\u001b[1;32m    455\u001b[0m \u001b[39m# Manage initial weight values if passed.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "n_episodes = 400\n",
    "max_iteration_ep = 500\n",
    "\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "total_steps = 0\n",
    "\n",
    "\n",
    "for e in range(n_episodes):\n",
    "    current_state = env.reset()\n",
    "    current_state = np.array([current_state])\n",
    "\n",
    "    for step in range(max_iteration_ep):\n",
    "        total_steps = total_steps+1\n",
    "\n",
    "        action = agent.compute_action(current_state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.array([next_state])\n",
    "\n",
    "        agent.store_episode(current_state, action, reward, next_state, done)\n",
    "\n",
    "\n",
    "        if done:\n",
    "            agent.update_exploration_probability()\n",
    "            break\n",
    "        current_state = next_state\n",
    "\n",
    "        if total_steps >= batch_size:\n",
    "            agent.train(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37869de",
   "metadata": {},
   "source": [
    "# Copy from website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6286205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.n_actions = action_size\n",
    "        # we define some parameters and hyperparameters:\n",
    "        # \"lr\" : learning rate\n",
    "        # \"gamma\": discounted factor\n",
    "        # \"exploration_proba_decay\": decay of the exploration probability\n",
    "        # \"batch_size\": size of experiences we sample to train the DNN\n",
    "        self.lr = 0.001\n",
    "        self.gamma = 0.99\n",
    "        self.exploration_proba = 1.0\n",
    "        self.exploration_proba_decay = 0.005\n",
    "        self.batch_size = 32\n",
    "        \n",
    "        # We define our memory buffer where we will store our experiences\n",
    "        # We stores only the 2000 last time steps\n",
    "        self.memory_buffer= list()\n",
    "        self.max_memory_buffer = 2000\n",
    "        \n",
    "        # We creaate our model having to hidden layers of 24 units (neurones)\n",
    "        # The first layer has the same size as a state size\n",
    "        # The last layer has the size of actions space\n",
    "        self.model = Sequential([\n",
    "            Dense(units=24,input_dim=state_size, activation = 'relu'),\n",
    "            Dense(units=24,activation = 'relu'),\n",
    "            Dense(units=action_size, activation = 'linear')\n",
    "        ])\n",
    "        self.model.compile(loss=\"mse\",\n",
    "                      optimizer = Adam(lr=self.lr))\n",
    "        \n",
    "    # The agent computes the action to perform given a state \n",
    "    def compute_action(self, current_state):\n",
    "        # We sample a variable uniformly over [0,1]\n",
    "        # if the variable is less than the exploration probability\n",
    "        #     we choose an action randomly\n",
    "        # else\n",
    "        #     we forward the state through the DNN and choose the action \n",
    "        #     with the highest Q-value.\n",
    "        if np.random.uniform(0,1) < self.exploration_proba:\n",
    "            return np.random.choice(range(self.n_actions))\n",
    "        q_values = self.model.predict(current_state)[0]\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "    # when an episode is finished, we update the exploration probability using \n",
    "    # espilon greedy algorithm\n",
    "    def update_exploration_probability(self):\n",
    "        self.exploration_proba = self.exploration_proba * np.exp(-self.exploration_proba_decay)\n",
    "        print(self.exploration_proba)\n",
    "    \n",
    "    # At each time step, we store the corresponding experience\n",
    "    def store_episode(self,current_state, action, reward, next_state, done):\n",
    "        #We use a dictionnary to store them\n",
    "        self.memory_buffer.append({\n",
    "            \"current_state\":current_state,\n",
    "            \"action\":action,\n",
    "            \"reward\":reward,\n",
    "            \"next_state\":next_state,\n",
    "            \"done\" :done\n",
    "        })\n",
    "        # If the size of memory buffer exceeds its maximum, we remove the oldest experience\n",
    "        if len(self.memory_buffer) > self.max_memory_buffer:\n",
    "            self.memory_buffer.pop(0)\n",
    "    \n",
    "\n",
    "    # At the end of each episode, we train our model\n",
    "    def train(self):\n",
    "        # We shuffle the memory buffer and select a batch size of experiences\n",
    "        np.random.shuffle(self.memory_buffer)\n",
    "        batch_sample = self.memory_buffer[0:self.batch_size]\n",
    "        \n",
    "        # We iterate over the selected experiences\n",
    "        for experience in batch_sample:\n",
    "            # We compute the Q-values of S_t\n",
    "            q_current_state = self.model.predict(experience[\"current_state\"])\n",
    "            # We compute the Q-target using Bellman optimality equation\n",
    "            q_target = experience[\"reward\"]\n",
    "            if not experience[\"done\"]:\n",
    "                q_target = q_target + self.gamma*np.max(self.model.predict(experience[\"next_state\"])[0])\n",
    "            q_current_state[0][experience[\"action\"]] = q_target\n",
    "            # train the model\n",
    "            self.model.fit(experience[\"current_state\"], q_current_state, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e786cef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "/Users/helloworld/anaconda3/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/Users/helloworld/anaconda3/lib/python3.11/site-packages/gym/envs/classic_control/cartpole.py:177: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/helloworld/Desktop/RL/Own.ipynb Cell 28\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X40sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     current_state \u001b[39m=\u001b[39m next_state\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X40sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# if the have at least batch_size experiences in the memory buffer\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X40sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# than we tain our model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X40sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mif\u001b[39;00m total_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m batch_size:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X40sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     agent\u001b[39m.\u001b[39mtrain(batch_size\u001b[39m=\u001b[39mbatch_size)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "# We create our gym environment \n",
    "env = gym.make(\"CartPole-v1\")\n",
    "# We get the shape of a state and the actions space size\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "# Number of episodes to run\n",
    "n_episodes = 400\n",
    "# Max iterations per epiode\n",
    "max_iteration_ep = 500\n",
    "# We define our agent\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "total_steps = 0\n",
    "\n",
    "# We iterate over episodes\n",
    "for e in range(n_episodes):\n",
    "    # We initialize the first state and reshape it to fit \n",
    "    #  with the input layer of the DNN\n",
    "    current_state = env.reset()\n",
    "    \n",
    "    for step in range(max_iteration_ep):\n",
    "        total_steps = total_steps + 1\n",
    "        # the agent computes the action to perform\n",
    "        action = agent.compute_action(current_state)\n",
    "        # the envrionment runs the action and returns\n",
    "        # the next state, a reward and whether the agent is done\n",
    "        next_state, reward, truncated, info, done  = env.step(action)\n",
    "        next_state = np.array([next_state])\n",
    "        \n",
    "        # We sotre each experience in the memory buffer\n",
    "        agent.store_episode(current_state, action, reward, next_state, done)\n",
    "        \n",
    "        # if the episode is ended, we leave the loop after\n",
    "        # updating the exploration probability\n",
    "        if done:\n",
    "            agent.update_exploration_probability()\n",
    "            break\n",
    "        current_state = next_state\n",
    "    # if the have at least batch_size experiences in the memory buffer\n",
    "    # than we tain our model\n",
    "    if total_steps >= batch_size:\n",
    "        agent.train(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec5a10c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83846656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([state_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34be40ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (1, 2) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/helloworld/Desktop/RL/Own.ipynb Cell 30\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m current_state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m current_state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([current_state])\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (1, 2) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "current_state = env.reset()\n",
    "current_state = np.array([current_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a71a8982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00747159, -0.01249113, -0.04659681,  0.03171724], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_state = env.reset()\n",
    "current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55a43466",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, truncated, info, done = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8eb38426",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/helloworld/Desktop/RL/Own.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m np\u001b[39m.\u001b[39marray(current_state)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "np.array(current_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f984ae9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wrappers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/helloworld/Desktop/RL/Own.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X52sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     env\u001b[39m.\u001b[39mclose()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X52sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     env_to_wrap\u001b[39m.\u001b[39mclose()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X52sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m make_video()\n",
      "\u001b[1;32m/Users/helloworld/Desktop/RL/Own.ipynb Cell 35\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_video\u001b[39m():\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X52sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     env_to_wrap \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m'\u001b[39m\u001b[39mCartPole-v1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X52sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     env \u001b[39m=\u001b[39m wrappers\u001b[39m.\u001b[39mMonitor(env_to_wrap, \u001b[39m'\u001b[39m\u001b[39mvideos\u001b[39m\u001b[39m'\u001b[39m, force \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X52sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     rewards \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X52sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     steps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wrappers' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def make_video():\n",
    "    env_to_wrap = gym.make('CartPole-v1')\n",
    "    env = wrappers.Monitor(env_to_wrap, 'videos', force = True)\n",
    "    rewards = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    state = np.array([state])\n",
    "    while not done:\n",
    "        action = agent.compute_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        state = np.array([state])            \n",
    "        steps += 1\n",
    "        rewards += reward\n",
    "    print(rewards)\n",
    "    env.close()\n",
    "    env_to_wrap.close()\n",
    "make_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a12457",
   "metadata": {},
   "source": [
    "# Building Another DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "809cdd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea8f46f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d80c15ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a97ece",
   "metadata": {},
   "source": [
    "A lower γ makes rewards from the uncertain far future less important for our agent than the ones in the near future that it can be fairly confident about. It also encourages agents to collect reward closer in time than equivalent rewards that are temporally far away in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78e2f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "afb5cae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "state, info = env.reset()\n",
    "n_observations  = len(state)\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9f4a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93143086",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
