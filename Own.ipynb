{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11208eff-9062-42af-a868-fde95ce3c9e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e59db2-9ab4-44bb-b7a0-8108fd461506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, truncated, terminated, info = env.step(action)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbb0742a-dbd6-4d97-9894-bee84fcd9a13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/Users/helloworld/anaconda3/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m      5\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m----> 6\u001b[0m     observation, reward, truncated, terminated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n\u001b[1;32m      9\u001b[0m         observation, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gym/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ale_py/env/gym.py:256\u001b[0m, in \u001b[0;36mAtariEnv.step\u001b[0;34m(self, action_ind)\u001b[0m\n\u001b[1;32m    254\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frameskip):\n\u001b[0;32m--> 256\u001b[0m     reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[1;32m    257\u001b[0m is_terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_over(with_truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    258\u001b[0m is_truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_truncated()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"ALE/Bowling-v5\", render_mode = \"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, truncated, terminated, info = env.step(action)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6130490-4035-471c-b6c5-12208be996fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[180, 122,  48],\n",
       "        [180, 122,  48],\n",
       "        [180, 122,  48],\n",
       "        ...,\n",
       "        [180, 122,  48],\n",
       "        [180, 122,  48],\n",
       "        [180, 122,  48]],\n",
       "\n",
       "       [[180, 122,  48],\n",
       "        [180, 122,  48],\n",
       "        [180, 122,  48],\n",
       "        ...,\n",
       "        [180, 122,  48],\n",
       "        [180, 122,  48],\n",
       "        [180, 122,  48]],\n",
       "\n",
       "       [[180, 122,  48],\n",
       "        [180, 122,  48],\n",
       "        [180, 122,  48],\n",
       "        ...,\n",
       "        [180, 122,  48],\n",
       "        [180, 122,  48],\n",
       "        [180, 122,  48]]], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "101f6ecd-ce02-4273-9e0a-c2f7338bf835",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lives': 0, 'episode_frame_number': 1024, 'frame_number': 1024}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562c063c-61f8-4eed-bbec-455ea962c9fd",
   "metadata": {},
   "source": [
    "# Observing Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8eb8f267-05b3-499b-a4f5-37ba5e212f1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Bowling-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b49a1be-6037-4221-b109-3d35c073cfc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "done = False\n",
    "observation, info = env.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "929c04e5-24bd-4d65-a4c6-d0495c870ee6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/helloworld/anaconda3/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:249: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "\n",
    "observation, reward, terminated,truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35c1b5c-d068-4fda-a86f-76034366719f",
   "metadata": {},
   "source": [
    "# Epsilon-Greedy Strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb6dd818-81b7-470a-8824-7a66cc9d9f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BowlingAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.95,\n",
    "    ):\n",
    "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
    "        of state-action values (q_values), a learning rate and an epsilon.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: The learning rate\n",
    "            initial_epsilon: The initial epsilon value\n",
    "            epsilon_decay: The decay for epsilon\n",
    "            final_epsilon: The final epsilon value\n",
    "            discount_factor: The discount factor for computing the Q-value\n",
    "        \"\"\"\n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.training_error = []\n",
    "\n",
    "    def get_action(self, obs: tuple[int, int, bool]) -> int:\n",
    "        \"\"\"\n",
    "        Returns the best action with probability (1 - epsilon)\n",
    "        otherwise a random action with probability epsilon to ensure exploration.\n",
    "        \"\"\"\n",
    "        # with probability epsilon return a random action to explore the environment\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "\n",
    "        # with probability (1 - epsilon) act greedily (exploit)\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        obs: tuple[int, int, bool],\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        terminated: bool,\n",
    "        next_obs: tuple[int, int, bool],\n",
    "    ):\n",
    "        \"\"\"Updates the Q-value of an action.\"\"\"\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
    "        temporal_difference = (\n",
    "            reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n",
    "        )\n",
    "\n",
    "        self.q_values[obs][action] = (\n",
    "            self.q_values[obs][action] + self.lr * temporal_difference\n",
    "        )\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbc1427b-0062-469e-a523-e6c378ba5fcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "learning_rate = 0.01\n",
    "n_episodes = 100_000\n",
    "start_epsilon = 1.0\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2)  # reduce the exploration over time\n",
    "final_epsilon = 0.1\n",
    "\n",
    "agent = BlackjackAgent(\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a20284a-46c9-4e64-8355-291cfbb64288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from gym.wrappers import RecordEpisodeStatistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3edbb850-0fca-483d-929e-4d357d590976",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/helloworld/Desktop/RL/Own.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m next_obs, reward, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# update the agent\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m agent\u001b[39m.\u001b[39mupdate(obs, action, reward, terminated, next_obs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# update if the environment is done and the current obs\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m done \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "\u001b[1;32m/Users/helloworld/Desktop/RL/Own.ipynb Cell 14\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     obs: \u001b[39mtuple\u001b[39m[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m, \u001b[39mbool\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     next_obs: \u001b[39mtuple\u001b[39m[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m, \u001b[39mbool\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m ):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Updates the Q-value of an action.\"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     future_q_value \u001b[39m=\u001b[39m (\u001b[39mnot\u001b[39;00m terminated) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_values[next_obs])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     temporal_difference \u001b[39m=\u001b[39m (\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m         reward \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscount_factor \u001b[39m*\u001b[39m future_q_value \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_values[obs][action]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_values[obs][action] \u001b[39m=\u001b[39m (\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_values[obs][action] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr \u001b[39m*\u001b[39m temporal_difference\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X16sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # play one episode\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # update the agent\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "        # update if the environment is done and the current obs\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "    agent.decay_epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb183b7-2cc6-46c4-bc85-3900cb97d117",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>, {})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defaultdict(lambda: np.zeros(env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f0560bd-1988-4bc0-8a16-f3b924df6802",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_blackjack = gym.make(\"Blackjack-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d904d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_bowling = gym.make(\"ALE/Bowling-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17b36b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(6)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_bowling.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0870f329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_blackjack.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28fed757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tuple(Discrete(32), Discrete(11), Discrete(2))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_blackjack.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cbc0ac40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (210, 160, 3), uint8)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_bowling.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e759d",
   "metadata": {},
   "source": [
    "# Deep Q-Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99c091b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3ac8a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.n_actions = action_size\n",
    "        self.lr = 0.001\n",
    "        self.gamma = 0.99\n",
    "        self.exploration_proba = 1.0\n",
    "        self.exploration_proba_decay = 0.005\n",
    "        self.batch_size = 32\n",
    "\n",
    "        self.memory_buffer = list()\n",
    "        self.max_memory_buffer = 2000\n",
    "\n",
    "        self.model =  Sequential([\n",
    "            Dense(24, input_shape = state_size, activation = 'relu'),\n",
    "            Dense(24, activation = 'relu'),\n",
    "            Dense(action_size, activation = 'linear')\n",
    "        ])\n",
    "\n",
    "        self.model.compile(loss=\"mse\", optimizer = Adam(lr=self.lr))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        def compute_action(self, current_state):\n",
    "            if np.random.uniform(0,1) < self.exploration_proba:\n",
    "                return np.random.choice(range(self.n_actions))\n",
    "            q_values = self.model.predict(current_state)\n",
    "            return np.argmax(q_values)\n",
    "        \n",
    "        def update_exploration_probability(self):\n",
    "            self.exploration_proba = self.exploration_proba * np.exp(-self.exploration_proba_decay) # epsilon-greedy formula\n",
    "\n",
    "        def store_episode(self, current_state, action, reward, next_state, done):\n",
    "            self.memory_buffer.append({\n",
    "                 \"current_state\": current_state,\n",
    "                \"action\":action,\n",
    "                \"reward\":reward,\n",
    "                \"next_state\":next_state,\n",
    "                \"done\" :done\n",
    "            })\n",
    "\n",
    "            if len(self.memory_buffer) > self.max_memory_buffer:\n",
    "                self.memory_buffer.pop()\n",
    "\n",
    "        def train(self):\n",
    "            np.random.shuffle(self.memory_buffer)\n",
    "            batch_sample = self.memory_buffer[0:self.batch_size]\n",
    "\n",
    "            for experience in batch_sample:\n",
    "                q_current_state = self.model.predict(experience[\"current_state\"])\n",
    "                q_target = experience[\"reward\"]\n",
    "\n",
    "                if not experience[\"done\"]:\n",
    "                    q_target = self.q_target + self.gamma*np.max(self.model.predict(experience[\"next_state\"])[0])\n",
    "                \n",
    "                q_current_state[0][experience[\"action\"]] = q_target\n",
    "\n",
    "                self.model.fit(experience[\"current_state\"], q_current_state, verbose = 0)\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96fc191c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/helloworld/Desktop/RL/Own.ipynb Cell 25\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m n_episodes \u001b[39m=\u001b[39m \u001b[39m400\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m max_iteration_ep \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m agent \u001b[39m=\u001b[39m DQNAgent(state_size, action_size)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m total_steps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_episodes):\n",
      "\u001b[1;32m/Users/helloworld/Desktop/RL/Own.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory_buffer \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_memory_buffer \u001b[39m=\u001b[39m \u001b[39m2000\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m  Sequential([\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     Dense(\u001b[39m24\u001b[39m, input_shape \u001b[39m=\u001b[39m state_size, activation \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     Dense(\u001b[39m24\u001b[39m, activation \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     Dense(action_size, activation \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m ])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m\"\u001b[39m, optimizer \u001b[39m=\u001b[39m Adam(lr\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X35sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_action\u001b[39m(\u001b[39mself\u001b[39m, current_state):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/dtensor/utils.py:96\u001b[0m, in \u001b[0;36mallow_initializer_layout.<locals>._wrap_function\u001b[0;34m(layer_instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[39mif\u001b[39;00m layout:\n\u001b[1;32m     94\u001b[0m             layout_args[variable_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_layout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m layout\n\u001b[0;32m---> 96\u001b[0m init_method(layer_instance, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     98\u001b[0m \u001b[39m# Inject the layout parameter after the invocation of __init__()\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39mfor\u001b[39;00m layout_param_name, layout \u001b[39min\u001b[39;00m layout_args\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:117\u001b[0m, in \u001b[0;36mDense.__init__\u001b[0;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39m@utils\u001b[39m\u001b[39m.\u001b[39mallow_initializer_layout\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    104\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    116\u001b[0m ):\n\u001b[0;32m--> 117\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(activity_regularizer\u001b[39m=\u001b[39mactivity_regularizer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    119\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munits \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(units) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(units, \u001b[39mint\u001b[39m) \u001b[39melse\u001b[39;00m units\n\u001b[1;32m    120\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munits \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/trackable/base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    205\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/engine/base_layer.py:452\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[0;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    451\u001b[0m             batch_size \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 452\u001b[0m         batch_input_shape \u001b[39m=\u001b[39m (batch_size,) \u001b[39m+\u001b[39m \u001b[39mtuple\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    453\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_input_shape \u001b[39m=\u001b[39m batch_input_shape\n\u001b[1;32m    455\u001b[0m \u001b[39m# Manage initial weight values if passed.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "n_episodes = 400\n",
    "max_iteration_ep = 500\n",
    "\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "total_steps = 0\n",
    "\n",
    "\n",
    "for e in range(n_episodes):\n",
    "    current_state = env.reset()\n",
    "    current_state = np.array([current_state])\n",
    "\n",
    "    for step in range(max_iteration_ep):\n",
    "        total_steps = total_steps+1\n",
    "\n",
    "        action = agent.compute_action(current_state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.array([next_state])\n",
    "\n",
    "        agent.store_episode(current_state, action, reward, next_state, done)\n",
    "\n",
    "\n",
    "        if done:\n",
    "            agent.update_exploration_probability()\n",
    "            break\n",
    "        current_state = next_state\n",
    "\n",
    "        if total_steps >= batch_size:\n",
    "            agent.train(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37869de",
   "metadata": {},
   "source": [
    "# Copy from website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6286205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.n_actions = action_size\n",
    "        # we define some parameters and hyperparameters:\n",
    "        # \"lr\" : learning rate\n",
    "        # \"gamma\": discounted factor\n",
    "        # \"exploration_proba_decay\": decay of the exploration probability\n",
    "        # \"batch_size\": size of experiences we sample to train the DNN\n",
    "        self.lr = 0.001\n",
    "        self.gamma = 0.99\n",
    "        self.exploration_proba = 1.0\n",
    "        self.exploration_proba_decay = 0.005\n",
    "        self.batch_size = 32\n",
    "        \n",
    "        # We define our memory buffer where we will store our experiences\n",
    "        # We stores only the 2000 last time steps\n",
    "        self.memory_buffer= list()\n",
    "        self.max_memory_buffer = 2000\n",
    "        \n",
    "        # We creaate our model having to hidden layers of 24 units (neurones)\n",
    "        # The first layer has the same size as a state size\n",
    "        # The last layer has the size of actions space\n",
    "        self.model = Sequential([\n",
    "            Dense(units=24,input_dim=state_size, activation = 'relu'),\n",
    "            Dense(units=24,activation = 'relu'),\n",
    "            Dense(units=action_size, activation = 'linear')\n",
    "        ])\n",
    "        self.model.compile(loss=\"mse\",\n",
    "                      optimizer = Adam(lr=self.lr))\n",
    "        \n",
    "    # The agent computes the action to perform given a state \n",
    "    def compute_action(self, current_state):\n",
    "        # We sample a variable uniformly over [0,1]\n",
    "        # if the variable is less than the exploration probability\n",
    "        #     we choose an action randomly\n",
    "        # else\n",
    "        #     we forward the state through the DNN and choose the action \n",
    "        #     with the highest Q-value.\n",
    "        if np.random.uniform(0,1) < self.exploration_proba:\n",
    "            return np.random.choice(range(self.n_actions))\n",
    "        q_values = self.model.predict(current_state)[0]\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "    # when an episode is finished, we update the exploration probability using \n",
    "    # espilon greedy algorithm\n",
    "    def update_exploration_probability(self):\n",
    "        self.exploration_proba = self.exploration_proba * np.exp(-self.exploration_proba_decay)\n",
    "        print(self.exploration_proba)\n",
    "    \n",
    "    # At each time step, we store the corresponding experience\n",
    "    def store_episode(self,current_state, action, reward, next_state, done):\n",
    "        #We use a dictionnary to store them\n",
    "        self.memory_buffer.append({\n",
    "            \"current_state\":current_state,\n",
    "            \"action\":action,\n",
    "            \"reward\":reward,\n",
    "            \"next_state\":next_state,\n",
    "            \"done\" :done\n",
    "        })\n",
    "        # If the size of memory buffer exceeds its maximum, we remove the oldest experience\n",
    "        if len(self.memory_buffer) > self.max_memory_buffer:\n",
    "            self.memory_buffer.pop(0)\n",
    "    \n",
    "\n",
    "    # At the end of each episode, we train our model\n",
    "    def train(self):\n",
    "        # We shuffle the memory buffer and select a batch size of experiences\n",
    "        np.random.shuffle(self.memory_buffer)\n",
    "        batch_sample = self.memory_buffer[0:self.batch_size]\n",
    "        \n",
    "        # We iterate over the selected experiences\n",
    "        for experience in batch_sample:\n",
    "            # We compute the Q-values of S_t\n",
    "            q_current_state = self.model.predict(experience[\"current_state\"])\n",
    "            # We compute the Q-target using Bellman optimality equation\n",
    "            q_target = experience[\"reward\"]\n",
    "            if not experience[\"done\"]:\n",
    "                q_target = q_target + self.gamma*np.max(self.model.predict(experience[\"next_state\"])[0])\n",
    "            q_current_state[0][experience[\"action\"]] = q_target\n",
    "            # train the model\n",
    "            self.model.fit(experience[\"current_state\"], q_current_state, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e786cef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "/Users/helloworld/anaconda3/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/Users/helloworld/anaconda3/lib/python3.11/site-packages/gym/envs/classic_control/cartpole.py:177: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/helloworld/Desktop/RL/Own.ipynb Cell 28\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X40sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     current_state \u001b[39m=\u001b[39m next_state\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X40sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# if the have at least batch_size experiences in the memory buffer\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X40sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# than we tain our model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X40sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mif\u001b[39;00m total_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m batch_size:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X40sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     agent\u001b[39m.\u001b[39mtrain(batch_size\u001b[39m=\u001b[39mbatch_size)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "# We create our gym environment \n",
    "env = gym.make(\"CartPole-v1\")\n",
    "# We get the shape of a state and the actions space size\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "# Number of episodes to run\n",
    "n_episodes = 400\n",
    "# Max iterations per epiode\n",
    "max_iteration_ep = 500\n",
    "# We define our agent\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "total_steps = 0\n",
    "\n",
    "# We iterate over episodes\n",
    "for e in range(n_episodes):\n",
    "    # We initialize the first state and reshape it to fit \n",
    "    #  with the input layer of the DNN\n",
    "    current_state = env.reset()\n",
    "    \n",
    "    for step in range(max_iteration_ep):\n",
    "        total_steps = total_steps + 1\n",
    "        # the agent computes the action to perform\n",
    "        action = agent.compute_action(current_state)\n",
    "        # the envrionment runs the action and returns\n",
    "        # the next state, a reward and whether the agent is done\n",
    "        next_state, reward, truncated, info, done  = env.step(action)\n",
    "        next_state = np.array([next_state])\n",
    "        \n",
    "        # We sotre each experience in the memory buffer\n",
    "        agent.store_episode(current_state, action, reward, next_state, done)\n",
    "        \n",
    "        # if the episode is ended, we leave the loop after\n",
    "        # updating the exploration probability\n",
    "        if done:\n",
    "            agent.update_exploration_probability()\n",
    "            break\n",
    "        current_state = next_state\n",
    "    # if the have at least batch_size experiences in the memory buffer\n",
    "    # than we tain our model\n",
    "    if total_steps >= batch_size:\n",
    "        agent.train(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec5a10c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83846656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([state_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34be40ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (1, 2) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/helloworld/Desktop/RL/Own.ipynb Cell 30\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m current_state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m current_state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([current_state])\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (1, 2) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "current_state = env.reset()\n",
    "current_state = np.array([current_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a71a8982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00747159, -0.01249113, -0.04659681,  0.03171724], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_state = env.reset()\n",
    "current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55a43466",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, truncated, info, done = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8eb38426",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/helloworld/Desktop/RL/Own.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m np\u001b[39m.\u001b[39marray(current_state)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "np.array(current_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f984ae9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wrappers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/helloworld/Desktop/RL/Own.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X52sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     env\u001b[39m.\u001b[39mclose()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X52sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     env_to_wrap\u001b[39m.\u001b[39mclose()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X52sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m make_video()\n",
      "\u001b[1;32m/Users/helloworld/Desktop/RL/Own.ipynb Cell 35\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_video\u001b[39m():\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X52sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     env_to_wrap \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m'\u001b[39m\u001b[39mCartPole-v1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X52sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     env \u001b[39m=\u001b[39m wrappers\u001b[39m.\u001b[39mMonitor(env_to_wrap, \u001b[39m'\u001b[39m\u001b[39mvideos\u001b[39m\u001b[39m'\u001b[39m, force \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X52sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     rewards \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/helloworld/Desktop/RL/Own.ipynb#X52sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     steps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wrappers' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def make_video():\n",
    "    env_to_wrap = gym.make('CartPole-v1')\n",
    "    env = wrappers.Monitor(env_to_wrap, 'videos', force = True)\n",
    "    rewards = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    state = np.array([state])\n",
    "    while not done:\n",
    "        action = agent.compute_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        state = np.array([state])            \n",
    "        steps += 1\n",
    "        rewards += reward\n",
    "    print(rewards)\n",
    "    env.close()\n",
    "    env_to_wrap.close()\n",
    "make_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a12457",
   "metadata": {},
   "source": [
    "# Building Another DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "809cdd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea8f46f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d80c15ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a97ece",
   "metadata": {},
   "source": [
    "A lower γ makes rewards from the uncertain far future less important for our agent than the ones in the near future that it can be fairly confident about. It also encourages agents to collect reward closer in time than equivalent rewards that are temporally far away in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78e2f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afb5cae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "state, info = env.reset()\n",
    "n_observations  = len(state)\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e9f4a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done \n",
    "    sample = random.random()\n",
    "\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1 * steps_done/ EPS_DECAY)\n",
    "\n",
    "    steps_done+=1\n",
    "\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "        \n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype = torch.long)\n",
    "    \n",
    "episode_durations = []\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0890bb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93143086",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    \n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de095f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a108c5f",
   "metadata": {},
   "source": [
    "# Analzying DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed3fb9b",
   "metadata": {},
   "source": [
    "Exploration - improving the current knowledge about each action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cdeaa5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
